import pandas as pd

# import utilities
from scipy.io import wavfile
from transformers.utils.dummy_pt_objects import torch_distributed_zero_first
import os
import w2v2_predict

from datasets import load_dataset, load_metric

import phoneme_asr as phoneme_asr

from transformers import Wav2Vec2Processor
from transformers import Wav2Vec2CTCTokenizer
from transformers import Wav2Vec2FeatureExtractor
from transformers import Wav2Vec2ForCTC
from transformers import TrainingArguments
from transformers import Trainer

import sys
import csv

import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn

from sklearn import metrics

import soundfile as sf
from multiprocessing.dummy import Pool as ThreadPool
from datetime import date, datetime
import pytz

def main():
    #######################
    # PARAM DECLARATION   #
    #######################
    bias = 0.5
    # bias = float(sys.argv[1])
    print(bias)
    #soft clean
    use_clean = True
    #False for midpoints, True for onset boundaries
    AIE_evaluation = False
    #hard clean
    clean_aggressive = True
    today_date = date.today()
    today_date_str = today_date.strftime("%d-%m-%Y") 
    #specify timezone
    set_timezone = pytz.timezone('Australia/Perth')
    today_time = datetime.now(set_timezone).time()
    today_time_str = today_time.strftime("%H:%M:%S") 
    print("Time = ", today_date_str, today_time_str) 
    experiment_name = str(bias)+"Bias-clean-"+str(use_clean)+"&"+str(clean_aggressive)+"-midpoint-UnsupSeg-"+today_date_str+"-"+today_time_str+"-26750ckpt"
    experiment_path = '/path/to/experiment/result/dir'+experiment_name

    if not os.path.isdir(experiment_path+'/'):
        os.mkdir(experiment_path)

    #Lets evaluate the ensembler
    ultrasuite = load_dataset("./ultrasuite", data_dir = "/path/to/UltraSuite/dataset")
    print(ultrasuite['train'][0])
    print(ultrasuite['test'][1])
    print(ultrasuite['test'][1]['file'])
    
    import json

    with open('/path/to/vocab.json') as vocab_file: 
        unicode_to_numeric_dict = json.loads(vocab_file.read())
    with open('/path/to/str_unic.json') as str_unic_file:
        str_to_unicode_dict = json.loads(str_unic_file.read())
    # for use with ultrasuite.map - add a unicode representation for each phoneme in the dataset
    def to_unicode_fn(batch):
        aux_lst = []
        for detailed_utterance in batch['phonetic_detail']:
            lst = []
            for phone in detailed_utterance['utterance']:
                lst.append(str_to_unicode_dict[phone])
            detailed_utterance['unic_utterance'] = lst[:]
            aux_lst.append(detailed_utterance)
        batch['phonetic_detail'] = aux_lst[:]
        return batch

    ultrasuite = ultrasuite.map(to_unicode_fn, batch_size=-1, batched=True)

    ## CONVERT LIST OF PHONES TO STRING OF PHONES
    def delim_phones_fn(batch):
        for detailed_utterance in batch['phonetic_detail']:
            detailed_utterance['string_utterance'] = ''.join(detailed_utterance['unic_utterance'])
        return batch
    ultrasuite = ultrasuite.map(delim_phones_fn, batch_size=-1, batched = True)

    ## CONVERSION TO 1D ARR
    def speech_file_to_array_fn(batch):
        speech_array, sampling_rate = sf.read(batch["file"])
        batch["speech"] = speech_array
        batch["sampling_rate"] = sampling_rate

        targ_seg_list = list()

        # the target_segs information will be gained from phonetic_detail features in ultrasuite/Torgo/UA_Speech/UltraSuite/SMAAT dataset,
        # the predicted results (generated by the model) will be compared with target_segs, and then we can evaluate the model performance
        for ii in range(len(batch['phonetic_detail']['start'])):
            targ_seg_list.append({"phone":batch['phonetic_detail']['utterance'][ii],
                                  "start":batch['phonetic_detail']['start'][ii]/sampling_rate,
                                  "stop":batch['phonetic_detail']['stop'][ii]/sampling_rate,
                                  "midpoint":(batch['phonetic_detail']['start'][ii]+batch['phonetic_detail']['stop'][ii])/(2*sampling_rate)
                                  })
        batch["target_segs"] = targ_seg_list
        return batch

    ultrasuite = ultrasuite.map(speech_file_to_array_fn, keep_in_memory=True, num_proc=8)
    ultrasuite

    # Get the tokens
    wp = w2v2_predict.w2v2_predictor()
    wp.set_model(ckpt="/path/to/finetuned/ckpt")
    def forcedAligner(wav_path, clean = use_clean, clean_agg = clean_aggressive):
        signalData, samplingFrequency = sf.read(wav_path)

        #Length of speech in seconds
        seconds = len(signalData) / samplingFrequency

        #Get the precollapse tokens
        tokens = wp.pred_wav_no_collapse(wav_path, return_type="phones")

        def tokens_to_timedtokens(signalData, samplingFrequency, tokens):
            # Duration of utterance in seconds
            seconds = len(signalData) / samplingFrequency

            # Delta s is half the distance in time between each token
            delta_s = seconds / (2 * len(tokens))

            # A list of tokens with time attached. It's called votelist because itll do some voting later on
            timed_token_list = list()

            # instantiate timestamp with one delta s. The distance between each token in time is 2 times delta_s
            timestamp = delta_s

            # This for loop creats a list of tuples with the timing attached to each
            for token in tokens:
                # Timed token is a tuple with the time in the sequence at which it occurs
                timed_token = (token, timestamp)

                # Add timed token to the voter list
                timed_token_list.append(timed_token)

                # Increment the timestamp for the next token
                timestamp = timestamp + 2 * delta_s
            
            return timed_token_list

        timed_token_list = tokens_to_timedtokens(signalData, samplingFrequency, tokens)
        
        # Now keep only the labels worth interpreting
        filtered_time_token_list = list()
        for tt in timed_token_list:
            if tt[0] != ("[PAD]" or "[UNK]" or "|"):
                filtered_time_token_list.append(tt)

        # Compute Decision Boundaries
        def decision_boundary_calc(filtered_time_token_list, seconds, bias = bias):
            assert 0 <= bias and bias <= 1
            DCB = list()
            for ii in range(len(filtered_time_token_list)):
                if ii == len(filtered_time_token_list) - 1:  # CASE: Last token
                    upper = seconds
                    lower = (filtered_time_token_list[ii - 1][1])*(1-bias) + (filtered_time_token_list[ii][1])*(bias)
                elif ii == 0:  # CASE: First token
                    upper = filtered_time_token_list[ii + 1][1]*(bias) + filtered_time_token_list[ii][1]*(1-bias)
                    lower = 0
                else:
                    upper = (filtered_time_token_list[ii + 1][1])*(bias) + (filtered_time_token_list[ii][1])*(1-bias)
                    lower = (filtered_time_token_list[ii - 1][1])*(1-bias) + (filtered_time_token_list[ii][1])*(bias)
                # append phone label, start time, end time tuple
                DCB.append((filtered_time_token_list[ii][0], lower, upper))
            
            return DCB
        DCB = decision_boundary_calc(filtered_time_token_list, seconds)
        Max_DCB_init_dict = dict.fromkeys(str_to_unicode_dict, 0)

        # Lets zip each label with its start and end times
        segList = list()
        [segList.append((DCB[ii][0], DCB[ii][1], DCB[ii][2])) for ii in range(len(DCB))]

        def clean_segs(segList_in, wav_path):
            segList = segList_in.copy()
            tokens_collapsed = wp.pred_wav_with_collapse(wav_path)

            transitions = list()
            for ii in range(len(tokens_collapsed) - 1):
                transitions.append((tokens_collapsed[ii], tokens_collapsed[ii + 1]))

            index = 0
            for jj in range(len(transitions)):
                found = False
                limitreached = False

                while found == False and limitreached == False:
                    if index >= len(segList) - 1:
                        limitreached = True
                    else:
                        seg_from = segList[index]
                        seg_to = segList[index + 1]

                        # CASE: the two elements are the same, ie seglist: aab, transition: ab, focal:aa, turn seglist into ab
                        if seg_from[0] == seg_to[0] and seg_from[0] == transitions[jj][0]:
                            segList[index] = (segList[index][0], segList[index][1], segList[index + 1][2])
                            segList.remove(segList[index + 1])


                        # CASE: Transition is found
                        #This should probably be transitions[jj+1][0], not transitions[jj][1]. Too late to change it.
                        elif seg_from[0] == transitions[jj][0] and seg_to[0] == transitions[jj][1]:
                            found = True
                            index = index + 1

                        else:
                            index = index + 1
                            break

            return segList

        def clean_segs_aggressive(segList_in, wav_path):
            segList = segList_in.copy()
            tokens_collapsed = wp.pred_wav_with_collapse(wav_path)

            transitions = list()
            for ii in range(len(tokens_collapsed) - 1):
                transitions.append((tokens_collapsed[ii], tokens_collapsed[ii + 1]))

            ceiling = len(segList) - 2
            jj = 0
            finished = False
            while finished == False:
                if jj <= ceiling:
                    if segList[jj][0] == segList[jj + 1][0]:
                        if not (segList[jj][0], segList[jj + 1][0]) in transitions:
                            newSeg = (segList[jj][0], segList[jj][1], segList[jj + 1][2])
                            segList[jj] = newSeg
                            segList.remove(segList[jj + 1])
                            ceiling = ceiling - 1
                            jj = jj - 1
                    jj = jj + 1
                else:
                    finished = True

            return segList

        if clean == True:
            if clean_agg == True:
                segList = clean_segs_aggressive(segList, wav_path)
                
            else:
                segList = clean_segs(segList, wav_path)

        temp = list()
        for seg in segList:
            temp.append({"phone": seg[0],
                         "start": seg[1],
                         "stop": seg[2]})
        segList = temp
        return segList

    def apply_forcedAligner(batch):
        batch["predict_segs"] = forcedAligner(batch["file"], clean = use_clean, clean_agg = clean_aggressive)
        return batch

    filename = experiment_path+"/processed_dataset.pickle"
    if not os.path.isfile(experiment_path+'/processed_dataset.pickle'):
        ultrasuite = ultrasuite["test"].map(apply_forcedAligner, num_proc = 1)
        ultrasuite

        outfile = open(filename, 'wb')
        pickle.dump(ultrasuite, outfile)
        outfile.close()
    else:
        infile = open(filename, 'rb')
        ultrasuite = pickle.load(infile)
        infile.close()



    def evaluate_results_map(batch):
        hits = 0
        for tseg in batch["target_segs"]:
            for pseg in batch["predict_segs"]:
                if pseg["start"] <= tseg["midpoint"] and tseg["midpoint"] <= pseg["stop"]:
                    pred_list.append({"Predicted":pseg["phone"],"Actual":tseg["phone"]})
                    if tseg["phone"] == pseg["phone"]:
                        hits = hits + 1
                        d_start = tseg["start"] - pseg["start"]
                        d_end = tseg["stop"] - pseg["stop"]
                        time_list.append({'delta_start':d_start, 'delta_end':d_end})

                elif tseg["stop"] < pseg["start"]:
                    break
        length1 = len(batch["target_segs"])
        length2 = len(batch["predict_segs"])
        recall = hits / length1
        acc = hits/length2
        utter_list.append({'recall':recall, 'accuracy':acc, 'hits':hits, 'groundtruth_length':length1, 'prediction_length':length2})

        return batch

    def evaluate_results_AIE_map(batch):
        
        hits = 0
        tau = (20*10**-3)
        for tseg in batch["target_segs"]:
            for pseg in batch["predict_segs"]:
                if abs(pseg["start"] - tseg["start"]) <= tau:
                    pred_list.append({"Predicted":pseg["phone"],"Actual":tseg["phone"]})
                    if tseg["phone"] == pseg["phone"]:
                        hits = hits + 1
                        d_start = tseg["start"] - pseg["start"]
                        time_list.append({'delta_start':d_start, 'delta_end':0})
                        break

                elif tseg["stop"] < pseg["start"]:
                    break
        length1 = len(batch["target_segs"])
        length2 = len(batch["predict_segs"])
        recall = hits / length1
        acc = hits/length2
        utter_list.append({'recall':recall, 'accuracy':acc, 'hits':hits, 'groundtruth_length':length1, 'prediction_length':length2})

        return batch

    if not os.path.isfile(experiment_path+'/pred_df.pickle') and not os.path.isfile(
            experiment_path+'/time_df.pickle') and not os.path.isfile(experiment_path+'/utter_df.pickle'):


        time_list = list()
        pred_list = list()
        utter_list = list()

        if AIE_evaluation == True:
            ultrasuite.map(evaluate_results_AIE_map)
        else:
            ultrasuite.map(evaluate_results_map)

        time_df = pd.DataFrame(time_list)
        utter_df = pd.DataFrame(utter_list)
        pred_df = pd.DataFrame(pred_list)

        filename = experiment_path+"/pred_df.pickle"
        outfile = open(filename, 'wb')
        pickle.dump(pred_df, outfile)
        outfile.close()

        filename = experiment_path+"/time_df.pickle"
        outfile = open(filename, 'wb')
        pickle.dump(time_df, outfile)
        outfile.close()

        filename = experiment_path+"/utter_df.pickle"
        outfile = open(filename, 'wb')
        pickle.dump(utter_df, outfile)
        outfile.close()

        pred_df.to_csv(experiment_path+"/pred_df.csv")
        time_df.to_csv(experiment_path+"/time_df.csv")
        utter_df.to_csv(experiment_path+"/utter_df.csv")


    else:
        filename = experiment_path+"/pred_df.pickle"
        infile = open(filename, 'rb')
        pred_df = pickle.load(infile)
        infile.close()

        filename = experiment_path+"/time_df.pickle"
        infile = open(filename, 'rb')
        time_df = pickle.load(infile)
        infile.close()

        filename = experiment_path+"/utter_df.pickle"
        infile = open(filename, 'rb')
        utter_df = pickle.load(infile)
        infile.close()

    print("Mean Accuracy: ", np.mean(utter_df["accuracy"]))
    plt.hist(abs(time_df["delta_start"]*1000),
             range = (0, 150),
             bins = 30,
             density=True,
             color = "skyblue",
             lw=0.1)
    plt.title("Absolute Time Difference of start times of matched and accurate predictions")
    plt.xlabel("Absolute difference in millisecons (ms)")
    plt.ylabel("Density")
    plt.savefig(experiment_path+"/starttimes")
    plt.show()

    plt.hist(abs(time_df["delta_end"]*1000),
             range = (0, 150),
             bins = 30,
             density=True,
             color = "skyblue",
             lw=1)
    plt.title("Absolute Time Difference of end times of matched and accurate predictions")
    plt.xlabel("Absolute difference in millisecons (ms)")
    plt.ylabel("Density")
    plt.savefig(experiment_path + "/endtimes")
    plt.show()

    
    labels = sorted(str_to_unicode_dict.keys()) 
    conf_matrix = metrics.confusion_matrix(pred_df["Actual"], pred_df["Predicted"], labels=labels)
    
    # Calculate the sum of each row
    row_sums = conf_matrix.sum(axis=1)

    # Extract the diagonal values
    diagonal_values = np.diag(conf_matrix)

    # Calculate the ratio of the diagonal values to the sum of the corresponding rows
    ratios = diagonal_values / row_sums

    # Combine class labels with their corresponding ratios
    results = list(zip(labels, ratios))

    # Output the results
    for label, ratio in results:
        print(f"{label}: {ratio}")

    df_cm = pd.DataFrame(conf_matrix, index=labels,
                         columns=labels)
    plt.figure(figsize=(15, 15))
    sn.heatmap(df_cm,
               annot=True,
               cmap="viridis", # Color map that provides good contrast
               linewidths=.5, # Small line widths to visually separate cells
               annot_kws={"size":7}, # Increase font size here as needed
               fmt="d") # Use 'd' for integer formatting, 'g' for compact number formatting
    plt.xlabel("Predicted Labels")
    plt.ylabel("Actual Labels")
    plt.title("Confusion matrix of matched predictions")

    print("")
    plt.savefig(experiment_path + "/confusion_matrix")
    plt.show()



    original_stdout = sys.stdout
    with open(experiment_path+'/metrics.txt', 'w') as f:
        sys.stdout = f
        print("Experiment Name:", experiment_name)
        print("Use clean:", use_clean)
        print("Clean Aggressively:", clean_aggressive)
        print("Bias:", bias)
        print("Eval. method: (AIE: true, SLP: false)", AIE_evaluation)



        print("Start times")
        print("Proportion less than 20ms:", len(time_df[time_df["delta_start"] <= 20/1000])/len(time_df))
        print("Proportion less than 40ms:", len(time_df[time_df["delta_start"] <= 40/1000])/len(time_df))
        print("Proportion less than 60ms:", len(time_df[time_df["delta_start"] <= 60/1000])/len(time_df))

        print("End times")
        print("Proportion less than 20ms:", len(time_df[time_df["delta_end"] <= 20 / 1000]) / len(time_df))
        print("Proportion less than 40ms:", len(time_df[time_df["delta_end"] <= 40 / 1000]) / len(time_df))
        print("Proportion less than 60ms:", len(time_df[time_df["delta_end"] <= 60 / 1000]) / len(time_df))
        print("")

        preds = 0
        for utter in ultrasuite["predict_segs"]:
            preds = preds + len(utter)

        msegs = 0 #Verification script
        for ii in ultrasuite["phonetic_detail"]:
            msegs = msegs + len(ii["utterance"])
        msegs

        npreds = sum(utter_df['prediction_length'])
        print("npreds: ", npreds)
        nmanualsegs= sum(utter_df['groundtruth_length'])
        print("nmanualsegs: ", nmanualsegs)
        test_df = pred_df[(pred_df['Predicted'] == pred_df['Actual'])]
        correctmatches = len(test_df)
        print("correctmatches: ", correctmatches)
        #Realistically, this accuracy is poorly named. While it is accuracy, its more like the precision
        # i.e. when I predict, what proportion am I getting right?
        precision = correctmatches / npreds
        print("Precision: ", precision)
        recall = correctmatches / nmanualsegs
        print("Recall: ", recall)
        print("Mean proportion of predictions correctly classified: ", np.mean(utter_df["accuracy"]))
        print("Mean proportion of ground truth correctly classified: ", np.mean(utter_df["recall"]))
        print("")
        print("Overall proportion of predictions correctly classified (Nhits / Npreds)", precision)
        print("Overall proportion of ground truth correctly classified (Nhits / Nmanualsegs)", recall)
        print("")
        print("Accuracy of predictions (%-Match):", len(test_df)/preds)
        # F1 score:
        hmean = (((recall)**-1+(precision)**-1)/2)**-1 
        print("Harmonic Mean Acc:", hmean)

        sys.stdout = original_stdout
        
        # export experimental results for various bias into a csv file
        # headers = ['Bias', 'Precision', 'Recall', 'Harmonic Mean', 'Start time', 'End time', 'Exp time']

        # data = {
        #     'Bias': bias,
        #     'Precision': precision,
        #     'Recall': recall,
        #     'Harmonic Mean': hmean,
        #     'Start time - Proportion less than 20ms': len(time_df[time_df["delta_start"] <= 20/1000])/len(time_df),
        #     'End time - Proportion less than 20ms': len(time_df[time_df["delta_end"] <= 20 / 1000]) / len(time_df),
        #     "Exp time": today_date_str +'-'+ today_time_str,
        # }

        # # Check if file exists and has content
        # file_path = "/path/to/csv/output/file"
        # file_exists = os.path.exists(file_path) and os.path.getsize(file_path) > 0

        # # Append data to the text file
        # with open(file_path, 'a', newline='') as file:
        #     writer = csv.writer(file)
            
        #     # If the file is new or empty, write the headers
        #     if not file_exists:
        #         writer.writerow(headers)
            
        #     # Write data values
        #     writer.writerow(data.values())

        # print("Data appended successfully.")


if __name__ == "__main__":
    main()
